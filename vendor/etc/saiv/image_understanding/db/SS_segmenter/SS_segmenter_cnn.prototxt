layer {
  name: "data"
  type: "Input"
  top: "data"
  input_param {
    shape {
      dim: 1
      dim: 3
      dim: 224
      dim: 224
    }
  }
}
layer {
  name: "MobilenetEdgeTPU/Conv/weights"
  type: "Convolution"
  bottom: "data"
  top: "MobilenetEdgeTPU/Conv/Relu"
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "MobilenetEdgeTPU/Conv/Relu/G2ACT"
  type: "ReLU"
  bottom: "MobilenetEdgeTPU/Conv/Relu"
  top: "MobilenetEdgeTPU/Conv/Relu"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv/project/weights"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/Conv/Relu"
  top: "MobilenetEdgeTPU/expanded_conv/project/BatchNorm/FusedBatchNormV3"
  convolution_param {
    num_output: 16
    bias_term: true
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_1/expand/weights"
  type: "Convolution"
  bottom: "MobilenetEdgeTPU/expanded_conv/project/BatchNorm/FusedBatchNormV3"
  top: "MobilenetEdgeTPU/expanded_conv_1/expand/Relu"
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_1/expand/Relu/G2ACT"
  type: "ReLU"
  bottom: "MobilenetEdgeTPU/expanded_conv_1/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_1/expand/Relu"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_1/project/weights"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_1/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_1/project/BatchNorm/FusedBatchNormV3"
  convolution_param {
    num_output: 32
    bias_term: true
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_2/expand/weights"
  type: "Convolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_1/project/BatchNorm/FusedBatchNormV3"
  top: "MobilenetEdgeTPU/expanded_conv_2/expand/Relu"
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_2/expand/Relu/G2ACT"
  type: "ReLU"
  bottom: "MobilenetEdgeTPU/expanded_conv_2/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_2/expand/Relu"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_2/project/weights"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_2/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_2/project/BatchNorm/FusedBatchNormV3"
  convolution_param {
    num_output: 32
    bias_term: true
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_2/add"
  type: "Eltwise"
  bottom: "MobilenetEdgeTPU/expanded_conv_2/project/BatchNorm/FusedBatchNormV3"
  bottom: "MobilenetEdgeTPU/expanded_conv_1/project/BatchNorm/FusedBatchNormV3"
  top: "MobilenetEdgeTPU/expanded_conv_2/add"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_3/expand/weights"
  type: "Convolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_2/add"
  top: "MobilenetEdgeTPU/expanded_conv_3/expand/Relu"
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_3/expand/Relu/G2ACT"
  type: "ReLU"
  bottom: "MobilenetEdgeTPU/expanded_conv_3/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_3/expand/Relu"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_3/project/weights"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_3/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_3/project/BatchNorm/FusedBatchNormV3"
  convolution_param {
    num_output: 32
    bias_term: true
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_3/add"
  type: "Eltwise"
  bottom: "MobilenetEdgeTPU/expanded_conv_3/project/BatchNorm/FusedBatchNormV3"
  bottom: "MobilenetEdgeTPU/expanded_conv_2/add"
  top: "MobilenetEdgeTPU/expanded_conv_3/add"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_4/expand/weights"
  type: "Convolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_3/add"
  top: "MobilenetEdgeTPU/expanded_conv_4/expand/Relu"
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_4/expand/Relu/G2ACT"
  type: "ReLU"
  bottom: "MobilenetEdgeTPU/expanded_conv_4/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_4/expand/Relu"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_4/project/weights"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_4/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_4/project/BatchNorm/FusedBatchNormV3"
  convolution_param {
    num_output: 32
    bias_term: true
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_4/add"
  type: "Eltwise"
  bottom: "MobilenetEdgeTPU/expanded_conv_4/project/BatchNorm/FusedBatchNormV3"
  bottom: "MobilenetEdgeTPU/expanded_conv_3/add"
  top: "MobilenetEdgeTPU/expanded_conv_4/add"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_5/expand/weights"
  type: "Convolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_4/add"
  top: "MobilenetEdgeTPU/expanded_conv_5/expand/Relu"
  convolution_param {
    num_output: 256
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 2
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_5/expand/Relu/G2ACT"
  type: "ReLU"
  bottom: "MobilenetEdgeTPU/expanded_conv_5/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_5/expand/Relu"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_5/project/weights"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_5/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_5/project/BatchNorm/FusedBatchNormV3"
  convolution_param {
    num_output: 48
    bias_term: true
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_6/expand/weights"
  type: "Convolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_5/project/BatchNorm/FusedBatchNormV3"
  top: "MobilenetEdgeTPU/expanded_conv_6/expand/Relu"
  convolution_param {
    num_output: 192
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_6/expand/Relu/G2ACT"
  type: "ReLU"
  bottom: "MobilenetEdgeTPU/expanded_conv_6/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_6/expand/Relu"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_6/project/weights"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_6/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_6/project/BatchNorm/FusedBatchNormV3"
  convolution_param {
    num_output: 48
    bias_term: true
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_6/add"
  type: "Eltwise"
  bottom: "MobilenetEdgeTPU/expanded_conv_6/project/BatchNorm/FusedBatchNormV3"
  bottom: "MobilenetEdgeTPU/expanded_conv_5/project/BatchNorm/FusedBatchNormV3"
  top: "MobilenetEdgeTPU/expanded_conv_6/add"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_7/expand/weights"
  type: "Convolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_6/add"
  top: "MobilenetEdgeTPU/expanded_conv_7/expand/Relu"
  convolution_param {
    num_output: 192
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_7/expand/Relu/G2ACT"
  type: "ReLU"
  bottom: "MobilenetEdgeTPU/expanded_conv_7/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_7/expand/Relu"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_7/project/weights"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_7/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_7/project/BatchNorm/FusedBatchNormV3"
  convolution_param {
    num_output: 48
    bias_term: true
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_7/add"
  type: "Eltwise"
  bottom: "MobilenetEdgeTPU/expanded_conv_7/project/BatchNorm/FusedBatchNormV3"
  bottom: "MobilenetEdgeTPU/expanded_conv_6/add"
  top: "MobilenetEdgeTPU/expanded_conv_7/add"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_8/expand/weights"
  type: "Convolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_7/add"
  top: "MobilenetEdgeTPU/expanded_conv_8/expand/Relu"
  convolution_param {
    num_output: 192
    bias_term: true
    pad: 1
    kernel_size: 3
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_8/expand/Relu/G2ACT"
  type: "ReLU"
  bottom: "MobilenetEdgeTPU/expanded_conv_8/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_8/expand/Relu"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_8/project/weights"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_8/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_8/project/BatchNorm/FusedBatchNormV3"
  convolution_param {
    num_output: 48
    bias_term: true
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_8/add"
  type: "Eltwise"
  bottom: "MobilenetEdgeTPU/expanded_conv_8/project/BatchNorm/FusedBatchNormV3"
  bottom: "MobilenetEdgeTPU/expanded_conv_7/add"
  top: "MobilenetEdgeTPU/expanded_conv_8/add"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_9/expand/weights"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_8/add"
  top: "MobilenetEdgeTPU/expanded_conv_9/expand/Relu"
  convolution_param {
    num_output: 384
    bias_term: true
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_9/expand/Relu/G2ACT"
  type: "ReLU"
  bottom: "MobilenetEdgeTPU/expanded_conv_9/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_9/expand/Relu"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_9/depthwise/depthwise_weights"
  type: "DepthwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_9/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_9/depthwise/Relu"
  convolution_param {
    num_output: 384
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 384
    stride: 2
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_9/depthwise/Relu/G2ACT"
  type: "ReLU"
  bottom: "MobilenetEdgeTPU/expanded_conv_9/depthwise/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_9/depthwise/Relu"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_9/project/weights"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_9/depthwise/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_9/project/BatchNorm/FusedBatchNormV3"
  convolution_param {
    num_output: 96
    bias_term: true
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_10/expand/weights"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_9/project/BatchNorm/FusedBatchNormV3"
  top: "MobilenetEdgeTPU/expanded_conv_10/expand/Relu"
  convolution_param {
    num_output: 384
    bias_term: true
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_10/expand/Relu/G2ACT"
  type: "ReLU"
  bottom: "MobilenetEdgeTPU/expanded_conv_10/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_10/expand/Relu"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_10/depthwise/depthwise_weights"
  type: "DepthwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_10/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_10/depthwise/Relu"
  convolution_param {
    num_output: 384
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 384
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_10/depthwise/Relu/G2ACT"
  type: "ReLU"
  bottom: "MobilenetEdgeTPU/expanded_conv_10/depthwise/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_10/depthwise/Relu"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_10/project/weights"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_10/depthwise/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_10/project/BatchNorm/FusedBatchNormV3"
  convolution_param {
    num_output: 96
    bias_term: true
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_10/add"
  type: "Eltwise"
  bottom: "MobilenetEdgeTPU/expanded_conv_10/project/BatchNorm/FusedBatchNormV3"
  bottom: "MobilenetEdgeTPU/expanded_conv_9/project/BatchNorm/FusedBatchNormV3"
  top: "MobilenetEdgeTPU/expanded_conv_10/add"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_11/expand/weights"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_10/add"
  top: "MobilenetEdgeTPU/expanded_conv_11/expand/Relu"
  convolution_param {
    num_output: 384
    bias_term: true
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_11/expand/Relu/G2ACT"
  type: "ReLU"
  bottom: "MobilenetEdgeTPU/expanded_conv_11/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_11/expand/Relu"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_11/depthwise/depthwise_weights"
  type: "DepthwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_11/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_11/depthwise/Relu"
  convolution_param {
    num_output: 384
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 384
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_11/depthwise/Relu/G2ACT"
  type: "ReLU"
  bottom: "MobilenetEdgeTPU/expanded_conv_11/depthwise/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_11/depthwise/Relu"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_11/project/weights"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_11/depthwise/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_11/project/BatchNorm/FusedBatchNormV3"
  convolution_param {
    num_output: 96
    bias_term: true
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_11/add"
  type: "Eltwise"
  bottom: "MobilenetEdgeTPU/expanded_conv_11/project/BatchNorm/FusedBatchNormV3"
  bottom: "MobilenetEdgeTPU/expanded_conv_10/add"
  top: "MobilenetEdgeTPU/expanded_conv_11/add"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_12/expand/weights"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_11/add"
  top: "MobilenetEdgeTPU/expanded_conv_12/expand/Relu"
  convolution_param {
    num_output: 384
    bias_term: true
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_12/expand/Relu/G2ACT"
  type: "ReLU"
  bottom: "MobilenetEdgeTPU/expanded_conv_12/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_12/expand/Relu"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_12/depthwise/depthwise_weights"
  type: "DepthwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_12/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_12/depthwise/Relu"
  convolution_param {
    num_output: 384
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 384
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_12/depthwise/Relu/G2ACT"
  type: "ReLU"
  bottom: "MobilenetEdgeTPU/expanded_conv_12/depthwise/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_12/depthwise/Relu"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_12/project/weights"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_12/depthwise/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_12/project/BatchNorm/FusedBatchNormV3"
  convolution_param {
    num_output: 96
    bias_term: true
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_12/add"
  type: "Eltwise"
  bottom: "MobilenetEdgeTPU/expanded_conv_12/project/BatchNorm/FusedBatchNormV3"
  bottom: "MobilenetEdgeTPU/expanded_conv_11/add"
  top: "MobilenetEdgeTPU/expanded_conv_12/add"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_13/expand/weights"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_12/add"
  top: "MobilenetEdgeTPU/expanded_conv_13/expand/Relu"
  convolution_param {
    num_output: 768
    bias_term: true
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_13/expand/Relu/G2ACT"
  type: "ReLU"
  bottom: "MobilenetEdgeTPU/expanded_conv_13/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_13/expand/Relu"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_13/depthwise/depthwise_weights"
  type: "DepthwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_13/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_13/depthwise/Relu"
  convolution_param {
    num_output: 768
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 768
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_13/depthwise/Relu/G2ACT"
  type: "ReLU"
  bottom: "MobilenetEdgeTPU/expanded_conv_13/depthwise/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_13/depthwise/Relu"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_13/project/weights"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_13/depthwise/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_13/project/BatchNorm/FusedBatchNormV3"
  convolution_param {
    num_output: 96
    bias_term: true
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_14/expand/weights"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_13/project/BatchNorm/FusedBatchNormV3"
  top: "MobilenetEdgeTPU/expanded_conv_14/expand/Relu"
  convolution_param {
    num_output: 384
    bias_term: true
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_14/expand/Relu/G2ACT"
  type: "ReLU"
  bottom: "MobilenetEdgeTPU/expanded_conv_14/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_14/expand/Relu"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_14/depthwise/depthwise_weights"
  type: "DepthwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_14/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_14/depthwise/Relu"
  convolution_param {
    num_output: 384
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 384
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_14/depthwise/Relu/G2ACT"
  type: "ReLU"
  bottom: "MobilenetEdgeTPU/expanded_conv_14/depthwise/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_14/depthwise/Relu"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_14/project/weights"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_14/depthwise/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_14/project/BatchNorm/FusedBatchNormV3"
  convolution_param {
    num_output: 96
    bias_term: true
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_14/add"
  type: "Eltwise"
  bottom: "MobilenetEdgeTPU/expanded_conv_14/project/BatchNorm/FusedBatchNormV3"
  bottom: "MobilenetEdgeTPU/expanded_conv_13/project/BatchNorm/FusedBatchNormV3"
  top: "MobilenetEdgeTPU/expanded_conv_14/add"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_15/expand/weights"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_14/add"
  top: "MobilenetEdgeTPU/expanded_conv_15/expand/Relu"
  convolution_param {
    num_output: 384
    bias_term: true
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_15/expand/Relu/G2ACT"
  type: "ReLU"
  bottom: "MobilenetEdgeTPU/expanded_conv_15/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_15/expand/Relu"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_15/depthwise/depthwise_weights"
  type: "DepthwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_15/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_15/depthwise/Relu"
  convolution_param {
    num_output: 384
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 384
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_15/depthwise/Relu/G2ACT"
  type: "ReLU"
  bottom: "MobilenetEdgeTPU/expanded_conv_15/depthwise/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_15/depthwise/Relu"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_15/project/weights"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_15/depthwise/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_15/project/BatchNorm/FusedBatchNormV3"
  convolution_param {
    num_output: 96
    bias_term: true
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_15/add"
  type: "Eltwise"
  bottom: "MobilenetEdgeTPU/expanded_conv_15/project/BatchNorm/FusedBatchNormV3"
  bottom: "MobilenetEdgeTPU/expanded_conv_14/add"
  top: "MobilenetEdgeTPU/expanded_conv_15/add"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_16/expand/weights"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_15/add"
  top: "MobilenetEdgeTPU/expanded_conv_16/expand/Relu"
  convolution_param {
    num_output: 384
    bias_term: true
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_16/expand/Relu/G2ACT"
  type: "ReLU"
  bottom: "MobilenetEdgeTPU/expanded_conv_16/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_16/expand/Relu"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_16/depthwise/depthwise_weights"
  type: "DepthwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_16/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_16/depthwise/Relu"
  convolution_param {
    num_output: 384
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 384
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_16/depthwise/Relu/G2ACT"
  type: "ReLU"
  bottom: "MobilenetEdgeTPU/expanded_conv_16/depthwise/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_16/depthwise/Relu"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_16/project/weights"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_16/depthwise/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_16/project/BatchNorm/FusedBatchNormV3"
  convolution_param {
    num_output: 96
    bias_term: true
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_16/add"
  type: "Eltwise"
  bottom: "MobilenetEdgeTPU/expanded_conv_16/project/BatchNorm/FusedBatchNormV3"
  bottom: "MobilenetEdgeTPU/expanded_conv_15/add"
  top: "MobilenetEdgeTPU/expanded_conv_16/add"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_17/expand/weights"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_16/add"
  top: "MobilenetEdgeTPU/expanded_conv_17/expand/Relu"
  convolution_param {
    num_output: 768
    bias_term: true
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_17/expand/Relu/G2ACT"
  type: "ReLU"
  bottom: "MobilenetEdgeTPU/expanded_conv_17/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_17/expand/Relu"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_17/depthwise/depthwise_weights"
  type: "DepthwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_17/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_17/depthwise/Relu"
  convolution_param {
    num_output: 768
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 768
    stride: 2
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_17/depthwise/Relu/G2ACT"
  type: "ReLU"
  bottom: "MobilenetEdgeTPU/expanded_conv_17/depthwise/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_17/depthwise/Relu"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_17/project/weights"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_17/depthwise/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_17/project/BatchNorm/FusedBatchNormV3"
  convolution_param {
    num_output: 160
    bias_term: true
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_18/expand/weights"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_17/project/BatchNorm/FusedBatchNormV3"
  top: "MobilenetEdgeTPU/expanded_conv_18/expand/Relu"
  convolution_param {
    num_output: 640
    bias_term: true
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_18/expand/Relu/G2ACT"
  type: "ReLU"
  bottom: "MobilenetEdgeTPU/expanded_conv_18/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_18/expand/Relu"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_18/depthwise/depthwise_weights"
  type: "DepthwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_18/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_18/depthwise/Relu"
  convolution_param {
    num_output: 640
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 640
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_18/depthwise/Relu/G2ACT"
  type: "ReLU"
  bottom: "MobilenetEdgeTPU/expanded_conv_18/depthwise/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_18/depthwise/Relu"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_18/project/weights"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_18/depthwise/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_18/project/BatchNorm/FusedBatchNormV3"
  convolution_param {
    num_output: 160
    bias_term: true
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_18/add"
  type: "Eltwise"
  bottom: "MobilenetEdgeTPU/expanded_conv_18/project/BatchNorm/FusedBatchNormV3"
  bottom: "MobilenetEdgeTPU/expanded_conv_17/project/BatchNorm/FusedBatchNormV3"
  top: "MobilenetEdgeTPU/expanded_conv_18/add"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_19/expand/weights"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_18/add"
  top: "MobilenetEdgeTPU/expanded_conv_19/expand/Relu"
  convolution_param {
    num_output: 640
    bias_term: true
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_19/expand/Relu/G2ACT"
  type: "ReLU"
  bottom: "MobilenetEdgeTPU/expanded_conv_19/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_19/expand/Relu"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_19/depthwise/depthwise_weights"
  type: "DepthwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_19/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_19/depthwise/Relu"
  convolution_param {
    num_output: 640
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 640
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_19/depthwise/Relu/G2ACT"
  type: "ReLU"
  bottom: "MobilenetEdgeTPU/expanded_conv_19/depthwise/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_19/depthwise/Relu"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_19/project/weights"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_19/depthwise/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_19/project/BatchNorm/FusedBatchNormV3"
  convolution_param {
    num_output: 160
    bias_term: true
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_19/add"
  type: "Eltwise"
  bottom: "MobilenetEdgeTPU/expanded_conv_19/project/BatchNorm/FusedBatchNormV3"
  bottom: "MobilenetEdgeTPU/expanded_conv_18/add"
  top: "MobilenetEdgeTPU/expanded_conv_19/add"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_20/expand/weights"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_19/add"
  top: "MobilenetEdgeTPU/expanded_conv_20/expand/Relu"
  convolution_param {
    num_output: 640
    bias_term: true
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_20/expand/Relu/G2ACT"
  type: "ReLU"
  bottom: "MobilenetEdgeTPU/expanded_conv_20/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_20/expand/Relu"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_20/depthwise/depthwise_weights"
  type: "DepthwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_20/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_20/depthwise/Relu"
  convolution_param {
    num_output: 640
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 640
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_20/depthwise/Relu/G2ACT"
  type: "ReLU"
  bottom: "MobilenetEdgeTPU/expanded_conv_20/depthwise/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_20/depthwise/Relu"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_20/project/weights"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_20/depthwise/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_20/project/BatchNorm/FusedBatchNormV3"
  convolution_param {
    num_output: 160
    bias_term: true
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_20/add"
  type: "Eltwise"
  bottom: "MobilenetEdgeTPU/expanded_conv_20/project/BatchNorm/FusedBatchNormV3"
  bottom: "MobilenetEdgeTPU/expanded_conv_19/add"
  top: "MobilenetEdgeTPU/expanded_conv_20/add"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_21/expand/weights"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_20/add"
  top: "MobilenetEdgeTPU/expanded_conv_21/expand/Relu"
  convolution_param {
    num_output: 768
    bias_term: true
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_21/expand/Relu/G2ACT"
  type: "ReLU"
  bottom: "MobilenetEdgeTPU/expanded_conv_21/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_21/expand/Relu"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_21/depthwise/depthwise_weights"
  type: "DepthwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_21/expand/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_21/depthwise/Relu"
  convolution_param {
    num_output: 768
    bias_term: true
    pad: 2
    kernel_size: 5
    group: 768
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_21/depthwise/Relu/G2ACT"
  type: "ReLU"
  bottom: "MobilenetEdgeTPU/expanded_conv_21/depthwise/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_21/depthwise/Relu"
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_21/project/weights"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_21/depthwise/Relu"
  top: "MobilenetEdgeTPU/expanded_conv_21/project/BatchNorm/FusedBatchNormV3"
  convolution_param {
    num_output: 160
    bias_term: true
    kernel_size: 1
    stride: 1
  }
}
layer {
  name: "MobilenetEdgeTPU/expanded_conv_21/add"
  type: "Eltwise"
  bottom: "MobilenetEdgeTPU/expanded_conv_21/project/BatchNorm/FusedBatchNormV3"
  bottom: "MobilenetEdgeTPU/expanded_conv_20/add"
  top: "MobilenetEdgeTPU/expanded_conv_21/add"
}
layer {
  name: "cls/pool/1"
  type: "Pooling"
  bottom: "MobilenetEdgeTPU/expanded_conv_21/add"
  top: "cls/pool/1"
  pooling_param {
    pool: AVE
    kernel_size: 7
    stride: 1
  }
}
layer {
  name: "cls/conv/1"
  type: "PointwiseConvolution"
  bottom: "cls/pool/1"
  top: "cls/conv/1"
  convolution_param {
    num_output: 640
    bias_term: true
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "cls/conv/2"
  type: "PointwiseConvolution"
  bottom: "cls/conv/1"
  top: "cls/conv/2"
  convolution_param {
    num_output: 1280
    bias_term: true
    kernel_size: 1
    stride: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "output_4"
  type: "PointwiseConvolution"
  bottom: "cls/conv/2"
  top: "output_4"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 2
    bias_term: true
    kernel_size: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "conv/c5/1x1"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_20/add"
  top: "conv/c5/1x1"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    kernel_size: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "relu/c5/1x1"
  type: "ReLU"
  bottom: "conv/c5/1x1"
  top: "conv/c5/1x1"
}
layer {
  name: "M5/x2"
  type: "Deconvolution"
  bottom: "conv/c5/1x1"
  top: "M5/x2"
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 1
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "conv/c4/1x1"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_16/add"
  top: "conv/c4/1x1"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    kernel_size: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "relu/c4/1x1"
  type: "ReLU"
  bottom: "conv/c4/1x1"
  top: "conv/c4/1x1"
}
layer {
  name: "M4"
  type: "Eltwise"
  bottom: "M5/x2"
  bottom: "conv/c4/1x1"
  top: "M4"
}
layer {
  name: "relu/M4"
  type: "ReLU"
  bottom: "M4"
  top: "M4"
}
layer {
  name: "M4/x2"
  type: "Deconvolution"
  bottom: "M4"
  top: "M4/x2"
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 1
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "conv/c3/1x1"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_8/add"
  top: "conv/c3/1x1"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    kernel_size: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "relu/c3/1x1"
  type: "ReLU"
  bottom: "conv/c3/1x1"
  top: "conv/c3/1x1"
}
layer {
  name: "M3"
  type: "Eltwise"
  bottom: "M4/x2"
  bottom: "conv/c3/1x1"
  top: "M3"
}
layer {
  name: "relu/M3"
  type: "ReLU"
  bottom: "M3"
  top: "M3"
}
layer {
  name: "M3/x2"
  type: "Deconvolution"
  bottom: "M3"
  top: "M3/x2"
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  convolution_param {
    num_output: 128
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 1
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "conv/c2/1x1"
  type: "PointwiseConvolution"
  bottom: "MobilenetEdgeTPU/expanded_conv_4/add"
  top: "conv/c2/1x1"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    kernel_size: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "relu/c2/1x1"
  type: "ReLU"
  bottom: "conv/c2/1x1"
  top: "conv/c2/1x1"
}
layer {
  name: "M2"
  type: "Eltwise"
  bottom: "M3/x2"
  bottom: "conv/c2/1x1"
  top: "M2"
}
layer {
  name: "relu/M2"
  type: "ReLU"
  bottom: "M2"
  top: "M2"
}
layer {
  name: "conv/M4/dwise"
  type: "DepthwiseConvolution"
  bottom: "M4"
  top: "conv/M4/dwise"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 128
    weight_filler {
      type: "msra"
    }
    engine: CAFFE
  }
}
layer {
  name: "relu/M4/dwise"
  type: "ReLU"
  bottom: "conv/M4/dwise"
  top: "conv/M4/dwise"
}
layer {
  name: "conv/M4/linear"
  type: "PointwiseConvolution"
  bottom: "conv/M4/dwise"
  top: "conv/M4/linear"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    kernel_size: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "relu/conv/M4/linear"
  type: "ReLU"
  bottom: "conv/M4/linear"
  top: "conv/M4/linear"
}
layer {
  name: "conv/M3/dwise"
  type: "DepthwiseConvolution"
  bottom: "M3"
  top: "conv/M3/dwise"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 128
    weight_filler {
      type: "msra"
    }
    engine: CAFFE
  }
}
layer {
  name: "relu/M3/dwise"
  type: "ReLU"
  bottom: "conv/M3/dwise"
  top: "conv/M3/dwise"
}
layer {
  name: "conv/M3/linear"
  type: "PointwiseConvolution"
  bottom: "conv/M3/dwise"
  top: "conv/M3/linear"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    kernel_size: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "relu/conv/M3/linear"
  type: "ReLU"
  bottom: "conv/M3/linear"
  top: "conv/M3/linear"
}
layer {
  name: "conv/M2/dwise"
  type: "DepthwiseConvolution"
  bottom: "M2"
  top: "conv/M2/dwise"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 128
    weight_filler {
      type: "msra"
    }
    engine: CAFFE
  }
}
layer {
  name: "relu/M2/dwise"
  type: "ReLU"
  bottom: "conv/M2/dwise"
  top: "conv/M2/dwise"
}
layer {
  name: "conv/M2/linear"
  type: "PointwiseConvolution"
  bottom: "conv/M2/dwise"
  top: "conv/M2/linear"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    kernel_size: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "relu/conv/M2/linear"
  type: "ReLU"
  bottom: "conv/M2/linear"
  top: "conv/M2/linear"
}
layer {
  name: "conv/P2/down/dwise"
  type: "DepthwiseConvolution"
  bottom: "conv/M2/linear"
  top: "conv/P2/down/dwise"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 128
    stride: 2
    weight_filler {
      type: "msra"
    }
    engine: CAFFE
  }
}
layer {
  name: "relu/P2/down/dwise"
  type: "ReLU"
  bottom: "conv/P2/down/dwise"
  top: "conv/P2/down/dwise"
}
layer {
  name: "P3/td"
  type: "Eltwise"
  bottom: "M3"
  bottom: "conv/M3/linear"
  bottom: "conv/P2/down/dwise"
  top: "P3/td"
}
layer {
  name: "conv/P3/down/dwise"
  type: "DepthwiseConvolution"
  bottom: "P3/td"
  top: "conv/P3/down/dwise"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 128
    stride: 2
    weight_filler {
      type: "msra"
    }
    engine: CAFFE
  }
}
layer {
  name: "relu/P3/down/dwise"
  type: "ReLU"
  bottom: "conv/P3/down/dwise"
  top: "conv/P3/down/dwise"
}
layer {
  name: "P4/td"
  type: "Eltwise"
  bottom: "M4"
  bottom: "conv/M4/linear"
  bottom: "conv/P3/down/dwise"
  top: "P4/td"
}
layer {
  name: "conv/SH2/1/dwise"
  type: "DepthwiseConvolution"
  bottom: "conv/M2/linear"
  top: "conv/SH2/1/dwise"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 128
    weight_filler {
      type: "msra"
    }
    engine: CAFFE
  }
}
layer {
  name: "relu/SH2/1/dwise"
  type: "ReLU"
  bottom: "conv/SH2/1/dwise"
  top: "conv/SH2/1/dwise"
}
layer {
  name: "conv/SH2/1/linear"
  type: "PointwiseConvolution"
  bottom: "conv/SH2/1/dwise"
  top: "conv/SH2/1/linear"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    kernel_size: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "relu/SH2/1/linear"
  type: "ReLU"
  bottom: "conv/SH2/1/linear"
  top: "conv/SH2/1/linear"
}
layer {
  name: "conv/SH/linear"
  type: "PointwiseConvolution"
  bottom: "conv/SH2/1/linear"
  top: "conv/SH/linear"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    kernel_size: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "relu/SH/linear"
  type: "ReLU"
  bottom: "conv/SH/linear"
  top: "conv/SH/linear"
}
layer {
  name: "SH/x2"
  type: "Deconvolution"
  bottom: "conv/SH/linear"
  top: "SH/x2"
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  convolution_param {
    num_output: 32
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 1
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "conv/SH/decoder/dwise"
  type: "DepthwiseConvolution"
  bottom: "SH/x2"
  top: "conv/SH/decoder/dwise"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 32
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 32
    weight_filler {
      type: "msra"
    }
    engine: CAFFE
  }
}
layer {
  name: "relu/SH/decoder/dwise"
  type: "ReLU"
  bottom: "conv/SH/decoder/dwise"
  top: "conv/SH/decoder/dwise"
}
layer {
  name: "conv/SH/decoder/linear"
  type: "PointwiseConvolution"
  bottom: "conv/SH/decoder/dwise"
  top: "conv/SH/decoder/linear"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 12
    bias_term: true
    kernel_size: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "relu/SH/decoder/linear"
  type: "ReLU"
  bottom: "conv/SH/decoder/linear"
  top: "conv/SH/decoder/linear"
}
layer {
  name: "prototypes_base"
  type: "Deconvolution"
  bottom: "conv/SH/decoder/linear"
  top: "prototypes_base"
  param {
    lr_mult: 0.0
    decay_mult: 0.0
  }
  convolution_param {
    num_output: 12
    bias_term: false
    pad: 1
    kernel_size: 4
    group: 1
    stride: 2
    weight_filler {
      type: "bilinear"
    }
  }
}
layer {
  name: "output_1"
  type: "PointwiseConvolution"
  bottom: "prototypes_base"
  top: "output_1"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 12
    bias_term: true
    kernel_size: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "conv/pred/P4/1/dwise"
  type: "DepthwiseConvolution"
  bottom: "P4/td"
  top: "conv/pred/P4/1/dwise"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 128
    weight_filler {
      type: "msra"
    }
    engine: CAFFE
  }
}
layer {
  name: "relu/pred/P4/1/dwise"
  type: "ReLU"
  bottom: "conv/pred/P4/1/dwise"
  top: "conv/pred/P4/1/dwise"
}
layer {
  name: "conv/pred/P4/1/linear"
  type: "PointwiseConvolution"
  bottom: "conv/pred/P4/1/dwise"
  top: "conv/pred/P4/1/linear"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    kernel_size: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "relu/pred/P4/1/linear"
  type: "ReLU"
  bottom: "conv/pred/P4/1/linear"
  top: "conv/pred/P4/1/linear"
}
layer {
  name: "conv/pred/P4/2/dwise"
  type: "DepthwiseConvolution"
  bottom: "conv/pred/P4/1/linear"
  top: "conv/pred/P4/2/dwise"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 128
    weight_filler {
      type: "msra"
    }
    engine: CAFFE
  }
}
layer {
  name: "relu/pred/P4/2/dwise"
  type: "ReLU"
  bottom: "conv/pred/P4/2/dwise"
  top: "conv/pred/P4/2/dwise"
}
layer {
  name: "conv/pred/P4/2/linear"
  type: "PointwiseConvolution"
  bottom: "conv/pred/P4/2/dwise"
  top: "conv/pred/P4/2/linear"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    kernel_size: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "relu/pred/P4/2/linear"
  type: "ReLU"
  bottom: "conv/pred/P4/2/linear"
  top: "conv/pred/P4/2/linear"
}
layer {
  name: "conv/pred/P4/3/dwise"
  type: "DepthwiseConvolution"
  bottom: "conv/pred/P4/2/linear"
  top: "conv/pred/P4/3/dwise"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 128
    weight_filler {
      type: "msra"
    }
    engine: CAFFE
  }
}
layer {
  name: "relu/pred/P4/3/dwise"
  type: "ReLU"
  bottom: "conv/pred/P4/3/dwise"
  top: "conv/pred/P4/3/dwise"
}
layer {
  name: "conv/pred/P4/3/linear"
  type: "PointwiseConvolution"
  bottom: "conv/pred/P4/3/dwise"
  top: "conv/pred/P4/3/linear"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    kernel_size: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "relu/pred/P4/3/linear"
  type: "ReLU"
  bottom: "conv/pred/P4/3/linear"
  top: "conv/pred/P4/3/linear"
}
layer {
  name: "conv/coeff/P4/dwise"
  type: "DepthwiseConvolution"
  bottom: "conv/pred/P4/3/linear"
  top: "conv/coeff/P4/dwise"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 128
    weight_filler {
      type: "msra"
    }
    engine: CAFFE
  }
}
layer {
  name: "relu/coeff/P4/dwise"
  type: "ReLU"
  bottom: "conv/coeff/P4/dwise"
  top: "conv/coeff/P4/dwise"
}
layer {
  name: "output_2"
  type: "PointwiseConvolution"
  bottom: "conv/coeff/P4/dwise"
  top: "output_2"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 12
    bias_term: true
    kernel_size: 1
    weight_filler {
      type: "msra"
    }
  }
}
layer {
  name: "conv/centerness/P4/dwise"
  type: "DepthwiseConvolution"
  bottom: "conv/pred/P4/3/linear"
  top: "conv/centerness/P4/dwise"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 128
    bias_term: true
    pad: 1
    kernel_size: 3
    group: 128
    weight_filler {
      type: "msra"
    }
    engine: CAFFE
  }
}
layer {
  name: "relu/centerness/P4/dwise"
  type: "ReLU"
  bottom: "conv/centerness/P4/dwise"
  top: "conv/centerness/P4/dwise"
}
layer {
  name: "output_3"
  type: "PointwiseConvolution"
  bottom: "conv/centerness/P4/dwise"
  top: "output_3"
  param {
    lr_mult: 1.0
    decay_mult: 1.0
  }
  convolution_param {
    num_output: 1
    bias_term: true
    kernel_size: 1
    weight_filler {
      type: "msra"
    }
  }
}
